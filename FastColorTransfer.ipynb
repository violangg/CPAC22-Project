{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms, datasets\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemError",
     "evalue": "GPU device not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[39mif\u001b[39;00m device\u001b[39m.\u001b[39mtype \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m----> 3\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mSystemError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mGPU device not found\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mSystemError\u001b[0m: GPU device not found"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if device.type != 'cuda':\n",
    "    raise SystemError('GPU device not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "STYLE_TRANSFORM_PATH = \"udnie_aggressive.pth\"\n",
    "PRESERVE_COLOR = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRANSFORMER.PY\n",
    "\n",
    "class TransformerNetwork(nn.Module):\n",
    "    \"\"\"Feedforward Transformation Network without Tanh\n",
    "    reference: https://arxiv.org/abs/1603.08155 \n",
    "    exact architecture: https://cs.stanford.edu/people/jcjohns/papers/fast-style/fast-style-supp.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(TransformerNetwork, self).__init__()\n",
    "        self.ConvBlock = nn.Sequential(\n",
    "            ConvLayer(3, 32, 9, 1),\n",
    "            nn.ReLU(),\n",
    "            ConvLayer(32, 64, 3, 2),\n",
    "            nn.ReLU(),\n",
    "            ConvLayer(64, 128, 3, 2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.ResidualBlock = nn.Sequential(\n",
    "            ResidualLayer(128, 3), \n",
    "            ResidualLayer(128, 3), \n",
    "            ResidualLayer(128, 3), \n",
    "            ResidualLayer(128, 3), \n",
    "            ResidualLayer(128, 3)\n",
    "        )\n",
    "        self.DeconvBlock = nn.Sequential(\n",
    "            DeconvLayer(128, 64, 3, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            DeconvLayer(64, 32, 3, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            ConvLayer(32, 3, 9, 1, norm=\"None\")\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ConvBlock(x)\n",
    "        x = self.ResidualBlock(x)\n",
    "        out = self.DeconvBlock(x)\n",
    "        return out\n",
    "\n",
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, norm=\"instance\"):\n",
    "        super(ConvLayer, self).__init__()\n",
    "        # Padding Layers\n",
    "        padding_size = kernel_size // 2\n",
    "        self.reflection_pad = nn.ReflectionPad2d(padding_size)\n",
    "\n",
    "        # Convolution Layer\n",
    "        self.conv_layer = nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
    "\n",
    "        # Normalization Layers\n",
    "        self.norm_type = norm\n",
    "        if (norm==\"instance\"):\n",
    "            self.norm_layer = nn.InstanceNorm2d(out_channels, affine=True)\n",
    "        elif (norm==\"batch\"):\n",
    "            self.norm_layer = nn.BatchNorm2d(out_channels, affine=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.reflection_pad(x)\n",
    "        x = self.conv_layer(x)\n",
    "        if (self.norm_type==\"None\"):\n",
    "            out = x\n",
    "        else:\n",
    "            out = self.norm_layer(x)\n",
    "        return out\n",
    "\n",
    "class ResidualLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Deep Residual Learning for Image Recognition\n",
    "\n",
    "    https://arxiv.org/abs/1512.03385\n",
    "    \"\"\"\n",
    "    def __init__(self, channels=128, kernel_size=3):\n",
    "        super(ResidualLayer, self).__init__()\n",
    "        self.conv1 = ConvLayer(channels, channels, kernel_size, stride=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = ConvLayer(channels, channels, kernel_size, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x                     # preserve residual\n",
    "        out = self.relu(self.conv1(x))   # 1st conv layer + activation\n",
    "        out = self.conv2(out)            # 2nd conv layer\n",
    "        out = out + identity             # add residual\n",
    "        return out\n",
    "\n",
    "class DeconvLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, output_padding, norm=\"instance\"):\n",
    "        super(DeconvLayer, self).__init__()\n",
    "\n",
    "        # Transposed Convolution \n",
    "        padding_size = kernel_size // 2\n",
    "        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding_size, output_padding)\n",
    "\n",
    "        # Normalization Layers\n",
    "        self.norm_type = norm\n",
    "        if (norm==\"instance\"):\n",
    "            self.norm_layer = nn.InstanceNorm2d(out_channels, affine=True)\n",
    "        elif (norm==\"batch\"):\n",
    "            self.norm_layer = nn.BatchNorm2d(out_channels, affine=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_transpose(x)\n",
    "        if (self.norm_type==\"None\"):\n",
    "            out = x\n",
    "        else:\n",
    "            out = self.norm_layer(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UTILS.PY\n",
    "\n",
    "# Load image file\n",
    "def load_image(path):\n",
    "    # Images loaded as BGR\n",
    "    img = cv2.imread(path)\n",
    "    return img\n",
    "\n",
    "# Show image\n",
    "def show(img):\n",
    "    # Convert from BGR to RGB\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # imshow() only accepts float [0,1] or int [0,255]\n",
    "    img = np.array(img/255).clip(0,1)\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "\n",
    "def saveimg(img, image_path):\n",
    "    img = img.clip(0, 255)\n",
    "    cv2.imwrite(image_path, img)\n",
    "\n",
    "# Preprocessing ~ Image to Tensor\n",
    "def itot(img, max_size=None):\n",
    "    # Rescale the image\n",
    "    if (max_size==None):\n",
    "        itot_t = transforms.Compose([\n",
    "            #transforms.ToPILImage(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Lambda(lambda x: x.mul(255))\n",
    "        ])    \n",
    "    else:\n",
    "        H, W, C = img.shape\n",
    "        image_size = tuple([int((float(max_size) / max([H,W]))*x) for x in [H, W]])\n",
    "        itot_t = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize(image_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Lambda(lambda x: x.mul(255))\n",
    "        ])\n",
    "\n",
    "    # Convert image to tensor\n",
    "    tensor = itot_t(img)\n",
    "\n",
    "    # Add the batch_size dimension\n",
    "    tensor = tensor.unsqueeze(dim=0)\n",
    "    return tensor\n",
    "\n",
    "# Preprocessing ~ Tensor to Image\n",
    "def ttoi(tensor):\n",
    "    # Add the means\n",
    "    #ttoi_t = transforms.Compose([\n",
    "    #    transforms.Normalize([-103.939, -116.779, -123.68],[1,1,1])])\n",
    "\n",
    "    # Remove the batch_size dimension\n",
    "    tensor = tensor.squeeze()\n",
    "    #img = ttoi_t(tensor)\n",
    "    img = tensor.cpu().numpy()\n",
    "    \n",
    "    # Transpose from [C, H, W] -> [H, W, C]\n",
    "    img = img.transpose(1, 2, 0)\n",
    "    return img\n",
    "\n",
    "def transfer_color(src, dest):\n",
    "    \"\"\"\n",
    "    Transfer Color using YIQ colorspace. Useful in preserving colors in style transfer.\n",
    "    This method assumes inputs of shape [Height, Width, Channel] in BGR Color Space\n",
    "    \"\"\"\n",
    "    src, dest = src.clip(0,255), dest.clip(0,255)\n",
    "        \n",
    "    # Resize src to dest's size\n",
    "    H,W,_ = src.shape \n",
    "    dest = cv2.resize(dest, dsize=(W, H), interpolation=cv2.INTER_CUBIC)\n",
    "    \n",
    "    dest_gray = cv2.cvtColor(dest, cv2.COLOR_BGR2GRAY) #1 Extract the Destination's luminance\n",
    "    src_yiq = cv2.cvtColor(src, cv2.COLOR_BGR2YCrCb)   #2 Convert the Source from BGR to YIQ/YCbCr\n",
    "    src_yiq[...,0] = dest_gray                         #3 Combine Destination's luminance and Source's IQ/CbCr\n",
    "    \n",
    "    return cv2.cvtColor(src_yiq, cv2.COLOR_YCrCb2BGR).clip(0,255)  #4 Convert new image from YIQ back to BGR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stylize():\n",
    "    # Device\n",
    "    device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load Transformer Network\n",
    "    net = TransformerNetwork()\n",
    "    net.load_state_dict(torch.load(STYLE_TRANSFORM_PATH, map_location=torch.device('cpu')))\n",
    "    net = net.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        while(1):\n",
    "            torch.cuda.empty_cache()\n",
    "            print(\"Stylize Image~ Press Ctrl+C and Enter to close the program\")\n",
    "            content_image_path = input(\"Enter the image path: \")\n",
    "            content_image = load_image(content_image_path)\n",
    "            starttime = time.time()\n",
    "            content_tensor = itot(content_image).to(device)\n",
    "            generated_tensor = net(content_tensor)\n",
    "            generated_image = ttoi(generated_tensor.detach())\n",
    "            if (PRESERVE_COLOR):\n",
    "                generated_image = transfer_color(content_image, generated_image)\n",
    "            print(\"Transfer Time: {}\".format(time.time() - starttime))\n",
    "            show(generated_image)\n",
    "            saveimg(generated_image, \"helloworld.jpg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('cpac-proj': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cd425c9899067e04fc55df916c7470f88412bb26de4f5a46a3579bebccfc5675"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
